#--------------------------------------------------------------------#
# -- INPUT/ OUTPUT
faunus.graph.input.format=org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat
#TODO -- move/ make driver arguments
faunus.input.location=/projects/lgst/25/complete


# output data (graph or statistic) parameters
faunus.graph.output.format=com.thinkaurelius.faunus.formats.titan.hbase.TitanHBaseOutputFormat

#-- for intermediate files only
#TODO -- move / make driver arguemtns
faunus.output.location=/projects/lgst/temp/output
faunus.output.location.overwrite=true

#-- HBASE
faunus.graph.output.titan.storage.backend=hbase
faunus.graph.output.titan.storage.hostname=localhost
faunus.graph.output.titan.storage.port=2181
faunus.graph.output.titan.storage.tablename=kronecker-25
faugus.graph.output.titan.storage.short-cf-names=true

#-- BATCH LOADING
faunus.graph.output.titan.storage.batch-loading=true
faunus.graph.output.titan.autotype = none

#-- These are set based on https://github.com/thinkaurelius/titan/wiki/Bulk-Loading
faunus.graph.output.titan.ids.block-size=250000
faunus.graph.output.titan.storage.idauthority-wait-time=200
faunus.graph.output.titan.storage.idauthority-retries = 1000


faunus.graph.output.titan.ids.renew-timeout = 450000
faunus.graph.output.titan.ids.partition=true
faunus.graph.output.titan.ids.num-partitions=24

# --
#Turning off the buffering
faunus.group.outout.titan.storage.storage.buffer-size=100000

#-------------------Metrics ---------------------------
faunus.graph.output.titan.storage.enable-basic-metrics = true

# Required; IP or hostname string
faunus.graph.output.titan.metrics.ganglia.addressing-mode = unicast
faunus.graph.output.titan.metrics.ganglia.hostname = localhost

# Required; specify logging interval in milliseconds
faunus.graph.output.titan.metrics.ganglia.interval = 10000
# Optional
faunus.graph.output.titan.metrics.ganglia.spoof = 10.0.0.1:titan.io

#-----------------------------------------------------------------------------#
#---------------- Lab41 specific Features ----------------
lab41.hbase.presplit = true
lab41.hbase.numberOfSplits = 320

#------------------------------------------------------------------------------#
#Basic Mapreduce Cofigurations
mapred.map.tasks.speculative.execution=false
mapred.reduce.tasks.speculative.execution=false

#-- Handles the Map Side Merge /Output
io.sort.factor=64
io.sort.record.percent=0.15
io.sort.spill.percent=0.98
io.sort.mb=512

# -- Handles the Reduce side Input
mapred.job.reduce.input.buffer.percent=0.8
mapred.inmem.merge.threshold=0
mapred.job.shuffle.merge.percent = 0.9

# ---- controls size of transaction (does it?)
mapred.max.split.size=67108864

mapred.reduce.slowstart.completed.maps=0.50

# -- Basic memory and Debug settings
mapred.map.child.java.opts=-Xmx1536m -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps
mapred.reduce.child.java.opts=-Xmx1536m -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps
mapred.job.reuse.jvm.num.tasks= 5

# -- Capacity Scheduler settings
# -- Need to set the max memory to be more than the heap size
# -- since we are using 1G slots for maps - requested 2 slots
# -- Since we are using 2G slots for reduces - requested 1 slots
mapred.job.map.memory.mb = 2048
mapred.job.reduce.memory.mb = 2048

# -- Compress both intermediate outputs and final
mapred.output.compress=true
mapred.compress.map.output=true

#Setting the hbase time out to accomodate table splits
hbase.rpc.timeout=240000

# Setting mapred time out to accomodate table split
mapred.task.timeout = 1200000


