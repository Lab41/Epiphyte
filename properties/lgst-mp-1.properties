#--------------------------------------------------------------------#
# -- INPUT/ OUTPUT
faunus.graph.input.format=org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat
faunus.input.location=/projects/lgst-new/29/undirected


# output data (graph or statistic) parameters
faunus.graph.output.format=com.thinkaurelius.faunus.formats.titan.hbase.TitanHBaseOutputFormat

#-- for intermediate files only (no side effects in these jobs)
faunus.output.location=/projects/lgst/temp/output
faunus.output.location.overwrite=true

#-- HBASE
faunus.graph.output.titan.storage.backend=hbase
faunus.graph.output.titan.storage.hostname=localhost
faunus.graph.output.titan.storage.port=2181
faunus.graph.output.titan.storage.tablename=kkr2-titan

#-- BATCH LOADING
faunus.graph.output.titan.storage.batch-loading=true
#-- These are set based on https://github.com/thinkaurelius/titan/wiki/Bulk-Loading
faunus.graph.output.titan.ids.block-size=250000
faunus.graph.output.titan.storage.idauthority-wait-time=500
faunus.graph.output.titan.storage.idauthority-retries = 200
faunus.group.outout.titan.storage.storage.buffer-size=20000

faunus.graph.output.titan.ids.renew-timeout = 200000
faunus.graph.output.titan.ids.partition=true
faunus.graph.output.titan.ids.num-partitions=24

#-- Subcluster features
faunus.graph.output.titan.storage.idauthority-unique-bits = 8
faunus.graph.output.titan.storage.idauthority-uniqueid-random = true
#faunus.graph.output.titan.storage.idauthority-local-consistency = true
#-------------------Metrics ---------------------------

faunus.graph.output.titan.storage.enable-basic-metrics = true

# Required; IP or hostname string
faunus.graph.output.titan.metrics.ganglia.addressing-mode = unicast
faunus.graph.output.titan.metrics.ganglia.hostname = localhost

# Required; specify logging interval in milliseconds
faunus.graph.output.titan.metrics.ganglia.interval = 15000
# Optional
faunus.graph.output.titan.metrics.ganglia.spoof = 10.0.0.1:Titan.com
#-----------------------------------------------------------------------------#
#---------------- Lab41 specific Features ----------------
lab41.hbase.presplit = true
lab41.hbase.numberOfSplits = 120

#------------------------------------------------------------------------------#
#Basic Mapreduce Cofigurations

#-- Handles the Map Side Merge /Output
io.sort.factor=64
io.sort.record.percent=0.15
io.sort.spill.percent=0.98
io.sort.mb=1024

# -- Handles the Reduce side Input
mapred.job.reduce.input.buffer.percent=0.8
mapred.inmem.merge.threshold=0
mapred.job.shuffle.merge.percent = 0.9
# ---- controls size of transaction (does it?)
mapred.max.split.size=134217728

# -- Turn off slow start - can end up utlizing all the memory on a cluster
mapred.reduce.slowstart.completed.maps=1.0
mapred.reduce.tasks=1000

# -- Basic memory and Debug settings
mapred.map.child.java.opts=-Xmx2G -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps
mapred.reduce.child.java.opts=-Xmx4G -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps
mapred.job.reuse.jvm.num.tasks=-1

# -- Compress both intermediate outputs and final
mapred.output.compress=true
mapred.compress.map.output=true

#Setting the hbase time out to accomodate createAndSplitTable.
hbase.rpc.timeout=240000

# Setting map red time out to accomodate splits: (2, min)
mapred.task.timeout = 1200000
